# MathFoundationRL

## 简介

西湖大学的课程，内容浅显易懂而且涉及到较为详细的推导，个人认为很适合用来作为强化学习的入门课。

[视频链接](https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.337.search-card.all.click&vd_source=b5280617363e9b17c3999fa2af42fee4)

[Github 链接](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning)

## 框架

- MDP 定义
- Bellman 方程
- 需要完整 model 的 DP（包括值迭代和策略迭代）
- 到无需完整 model 的 MC
- 引入 TD, Sarsa, Q-learning
- 当状态空间过大无法使用表格表示时，使用函数近似方法（如 DQN）
- 最后是直接优化策略的策略梯度方法和结合值函数与策略优化的 Actor-Critic 方法

## 我的笔记

[1 Basic Concepts](/CS-learning/assets/pdfs/MathFoundationRL/my-notes/1 Basic Concepts.pdf)

[2 State Values and Bellman Equation](/CS-learning/assets/pdfs/MathFoundationRL/my-notes/2 State Values and Bellman Equation.pdf)

[3 Optimal State Values and Bellman Optimality Equation](/CS-learning/assets/pdfs/MathFoundationRL/my-notes/3 Optimal State Values and Bellman Optimality Equation.pdf)

[4 Value Iteration and Policy Iteration](/CS-learning/assets/pdfs/MathFoundationRL/my-notes/4 Value Iteration and Policy Iteration.pdf)

[5 Monte Carlo Methods](/CS-learning/assets/pdfs/MathFoundationRL/my-notes/5 Monte Carlo Methods.pdf)

[6 Stochastic Approximation](/CS-learn
ing/assets/pdfs/MathFoundationRL/my-notes/6 Stochastic Approximation.pdf)

[7 Temporal-Difference Methods](/CS-learning/assets/pdfs/MathFoundationRL/my-notes/7 Temporal-Difference Methods.pdf)

[8 Value Function Approximation](/CS-learning/assets/pdfs/MathFoundationRL/my-notes/8 Value Function Approximation.pdf)

[9 Policy Gradient Methods](/CS-learning/assets/pdfs/MathFoundationRL/my-notes/9 Policy Gradient Methods.pdf)

[10 Actor-Critic Methods](/CS-learning/assets/pdfs/MathFoundationRL/my-notes/10 Actor-Critic Methods.pdf)
